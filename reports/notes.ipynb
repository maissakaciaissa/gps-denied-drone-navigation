{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfddce8",
   "metadata": {},
   "source": [
    "# Drone Visual Navigation Project\n",
    "\n",
    "## Backend Structure Overview\n",
    "\n",
    "---\n",
    "\n",
    "## PART 1: Environment Representation\n",
    "\n",
    "**What it is:** The world where the drone operates\n",
    "\n",
    "**What you need to code:**\n",
    "\n",
    "- Grid/Map system (2D or 3D coordinates)\n",
    "- Obstacles placement\n",
    "- Goal/destination location\n",
    "- Drone starting position\n",
    "\n",
    "---\n",
    "\n",
    "## PART 2: Drone Model\n",
    "\n",
    "**What it is:** The drone's properties and capabilities\n",
    "\n",
    "**What you need to code:**\n",
    "\n",
    "- Current position\n",
    "- Battery level\n",
    "- Movement capabilities (speed, range)\n",
    "- Sensor range (how far it can \"see\")\n",
    "\n",
    "---\n",
    "\n",
    "## PART 3: Game Theory Components\n",
    "\n",
    "### Players\n",
    "\n",
    "- **Drone** (decision-maker)\n",
    "- **Environment** (nature/adversary)\n",
    "\n",
    "### Strategies/Actions\n",
    "\n",
    "#### DRONE STRATEGIES\n",
    "\n",
    "**Pure strategies:**\n",
    "\n",
    "```\n",
    "{move_up, move_down, move_left, move_right, stay, rotate}\n",
    "```\n",
    "\n",
    "**Mixed strategy (σ_drone):**\n",
    "Probability distribution over pure strategies\n",
    "\n",
    "_Example:_\n",
    "\n",
    "```\n",
    "σ = (0.4·move_forward, 0.3·move_left, 0.2·move_right, 0.1·stay)\n",
    "```\n",
    "\n",
    "#### ENVIRONMENT STRATEGIES\n",
    "\n",
    "**Pure strategies:**\n",
    "\n",
    "```\n",
    "{clear_path, obstacle_ahead, low_visibility, sensor_noise, lighting_change}\n",
    "```\n",
    "\n",
    "**Mixed strategy (σ_env):**\n",
    "Probability distribution representing environmental uncertainty\n",
    "\n",
    "_Example:_\n",
    "\n",
    "```\n",
    "σ = (0.5·clear, 0.3·obstacle, 0.2·low_vis)\n",
    "```\n",
    "\n",
    "### Payoff Function\n",
    "\n",
    "```\n",
    "u_drone(s_drone, s_env) = w1·mission_success\n",
    "                        - w2·energy_consumed\n",
    "                        - w3·collision_risk\n",
    "                        + w4·map_quality\n",
    "```\n",
    "\n",
    "**Constraint:** `w1 + w2 + w3 + w4 = 1` (weights sum to 1)\n",
    "\n",
    "#### Detailed Component Calculations\n",
    "\n",
    "**mission_success:**\n",
    "\n",
    "- `1.0` if reached goal\n",
    "- `distance_to_goal / initial_distance` (partial credit)\n",
    "- `0` if collision\n",
    "\n",
    "**energy_consumed:**\n",
    "\n",
    "- `battery_used / total_battery`\n",
    "- Each move costs energy\n",
    "\n",
    "**collision_risk:**\n",
    "\n",
    "- `distance_to_nearest_obstacle^(-1)`\n",
    "- Higher when close to obstacles\n",
    "\n",
    "**map_quality:**\n",
    "\n",
    "- `unexplored_area / total_area`\n",
    "- Rewards exploration\n",
    "\n",
    "#### Example Payoff Matrix (2×2)\n",
    "\n",
    "|             | **Clear** | **Obstacle** |\n",
    "| ----------- | --------- | ------------ |\n",
    "| **Forward** | (10, -5)  | (-20, 5)     |\n",
    "| **Stay**    | (2, 0)    | (2, 0)       |\n",
    "\n",
    "Format: `(drone_payoff, env_payoff)`\n",
    "\n",
    "---\n",
    "\n",
    "## PART 4: Decision-Making Algorithms\n",
    "\n",
    "### Algorithm 1: Minimax (Adversarial Environment)\n",
    "\n",
    "**Assumptions:**\n",
    "\n",
    "1. Adversarial environment (worst-case scenario)\n",
    "2. Drone minimizes the maximum loss the environment can cause\n",
    "3. Related to dominant strategies\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "```\n",
    "s*_drone = arg max    min    u_drone(s_drone, s_env)\n",
    "           s_drone  s_env ∈ S_env\n",
    "```\n",
    "\n",
    "**Translation:** \"Choose the action that gives the best payoff even if environment picks the worst response\"\n",
    "\n",
    "### Algorithm 2: Nash Equilibrium\n",
    "\n",
    "For finding stable strategies\n",
    "\n",
    "### Algorithm 3: Bayesian Game\n",
    "\n",
    "For uncertainty handling and incomplete information\n",
    "\n",
    "---\n",
    "\n",
    "## HYBRID STRATEGY\n",
    "\n",
    "### Phase 1: Initial Navigation (high uncertainty)\n",
    "\n",
    "- Use **BAYESIAN GAME** with cautious priors\n",
    "- Gather sensor information\n",
    "\n",
    "### Phase 2: Known Dangerous Areas\n",
    "\n",
    "- Switch to **MINIMAX** (adversarial)\n",
    "- Guarantee safety\n",
    "\n",
    "### Phase 3: Open Areas\n",
    "\n",
    "- Use **NASH EQUILIBRIUM** (mixed)\n",
    "- Balance speed vs. safety optimally\n",
    "\n",
    "### Phase 4: Near Goal\n",
    "\n",
    "- Pure strategy (dominant)\n",
    "- Direct path if clear\n",
    "\n",
    "---\n",
    "\n",
    "## PART 5: Simulation Engine\n",
    "\n",
    "Main simulation loop implementation\n",
    "\n",
    "---\n",
    "\n",
    "## PART 6: Performance Metrics\n",
    "\n",
    "**What it is:** Measuring how well your solution works\n",
    "\n",
    "### Metrics Available\n",
    "\n",
    "#### Pareto Optimality\n",
    "\n",
    "A solution is Pareto optimal if no other solution improves one objective without worsening another\n",
    "\n",
    "_Check:_ Can we reach goal faster WITHOUT using more battery?\n",
    "\n",
    "#### Security Level\n",
    "\n",
    "```\n",
    "min_security_level = min    u_drone(s_drone, s_env)\n",
    "                     s_env\n",
    "```\n",
    "\n",
    "Worst-case guaranteed payoff\n",
    "\n",
    "#### Nash Equilibrium Quality\n",
    "\n",
    "```\n",
    "Efficiency = u(Nash) / u(Optimal)\n",
    "```\n",
    "\n",
    "Compare: Payoff at Nash vs. Optimal possible payoff\n",
    "\n",
    "#### Basic Metrics\n",
    "\n",
    "- Success rate (did it reach goal?)\n",
    "- Path efficiency (shortest path vs actual path)\n",
    "- Battery usage\n",
    "- Collision count\n",
    "- Time taken\n",
    "\n",
    "**Example metrics object:**\n",
    "\n",
    "```python\n",
    "metrics = {\n",
    "    'success': True/False,\n",
    "    'path_length': 15,\n",
    "    'battery_used': 45,\n",
    "    'collisions': 0,\n",
    "    'computation_time': 2.5  # seconds\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## PART 7: Comparison\n",
    "\n",
    "Compare approaches with and without game theory:\n",
    "\n",
    "```python\n",
    "results_with_game_theory = run_simulation(use_minimax=True)\n",
    "results_without = run_simulation(use_minimax=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Important Remarks\n",
    "\n",
    "### Algorithm Selection Guide\n",
    "\n",
    "| Algorithm            | Environment Type   | Use Case                                                                    |\n",
    "| -------------------- | ------------------ | --------------------------------------------------------------------------- |\n",
    "| **MINIMAX**          | Adversarial        | Worst-case thinking, environment actively opposes drone                     |\n",
    "| **NASH EQUILIBRIUM** | Neutral/Stochastic | Realistic scenarios, environment doesn't \"try\" to hurt drone                |\n",
    "| **BAYESIAN**         | Unknown type       | Incomplete information, drone doesn't know true environment state initially |\n",
    "\n",
    "---\n",
    "\n",
    "## Project File Structure\n",
    "\n",
    "```\n",
    "backend/\n",
    "│\n",
    "├── core/\n",
    "│   ├── environment.py      # PART 1: Grid, obstacles\n",
    "│   ├── drone.py            # PART 2: Drone model\n",
    "│   └── sensor.py           # Drone vision simulation\n",
    "│\n",
    "├── game_theory/\n",
    "│   ├── payoff.py           # PART 3C: Payoff function\n",
    "│   ├── minimax.py          # PART 4: Minimax algorithm\n",
    "│   ├── nash.py             # PART 4: Nash equilibrium\n",
    "│   └── strategies.py       # PART 3B: All strategies\n",
    "│\n",
    "├── simulation/\n",
    "│   ├── engine.py           # PART 5: Main simulation loop\n",
    "│   ├── metrics.py          # PART 6: Performance tracking\n",
    "│   └── logger.py           # Data logging\n",
    "│\n",
    "├── evaluation/\n",
    "│   └── compare.py          # PART 7: Compare approaches\n",
    "│\n",
    "└── main.py                 # Run everything\n",
    "```\n",
    "\n",
    "# Drone Visual Navigation Project - File Descriptions\n",
    "\n",
    "## PERSON 1 FILES\n",
    "\n",
    "### `backend/core/environment.py`\n",
    "\n",
    "This file contains the **Environment** class that represents the world where the drone operates.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- A grid system (like a chessboard) with specific width and height\n",
    "- A list of all obstacles placed on the grid (walls, buildings, etc.)\n",
    "- The goal position - where the drone needs to reach\n",
    "- The starting position - where the drone begins\n",
    "- Methods to check if a position is valid (not out of bounds, not hitting an obstacle)\n",
    "- A function to calculate distance from any position to the goal\n",
    "- A function to find the nearest obstacle from any given position\n",
    "- A method to get the current state of the environment (snapshot of everything)\n",
    "\n",
    "**Example:** If you have a 20x20 grid, this file lets you say \"put an obstacle at position (5,5)\" and \"the goal is at (18,18)\". It also answers questions like \"Is position (10,10) safe to move to?\" or \"How far is the drone from the goal?\"\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/game_theory/strategies.py`\n",
    "\n",
    "This file defines all possible actions (strategies) for both the drone and the environment.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "**For the Drone:**\n",
    "\n",
    "- A list of all pure strategies (single actions): move up, move down, move left, move right, stay in place, rotate\n",
    "- A method to create mixed strategies (probability distributions over actions). For example: \"40% chance move forward, 30% move left, 20% move right, 10% stay\"\n",
    "- A function to randomly sample one action from a mixed strategy based on the probabilities\n",
    "\n",
    "**For the Environment:**\n",
    "\n",
    "- A list of environmental conditions that can occur: clear path, obstacle suddenly appears, low visibility (fog/darkness), sensor noise (interference), lighting changes\n",
    "- Similar methods to create mixed strategies for the environment (probability distributions of conditions)\n",
    "- A function to sample which environmental condition actually occurs\n",
    "\n",
    "**Example:** The drone might have a cautious strategy like \"60% stay, 40% move forward\" when uncertain. The environment might have \"70% clear path, 20% obstacle, 10% low visibility\" to represent typical conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/game_theory/payoff.py`\n",
    "\n",
    "This file contains the **PayoffFunction** class that calculates how good or bad each decision is.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- Four weight parameters (w1, w2, w3, w4) that must sum to 1.0, representing the importance of each component\n",
    "- A function to calculate **mission success**: Returns 1.0 if the goal is reached, a partial score based on how close you got, or 0 if there was a collision\n",
    "- A function to calculate **energy consumed**: Returns the fraction of battery used (0 to 1)\n",
    "- A function to calculate **collision risk**: Returns a danger score based on how close the drone is to obstacles (closer = higher risk)\n",
    "- A function to calculate **map quality**: Returns an exploration score - how much of the map has been explored\n",
    "- The main **compute_payoff** function that combines all four components using the weights to give a single number representing \"how good is this situation\"\n",
    "- A method to generate a complete payoff matrix showing payoffs for every possible combination of drone action and environment condition\n",
    "\n",
    "**Example:** If the drone moves forward but there's an obstacle ahead, the payoff might be: (0.4 × 0) - (0.2 × 0.1) - (0.3 × 0.8) + (0.1 × 0.2) = -0.22 (negative means bad decision). If it stays safe and makes progress, the payoff might be +0.6 (positive means good decision).\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/simulation/logger.py` (Optional Helper)\n",
    "\n",
    "This file handles recording and saving simulation data.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- A logger class that keeps track of what happens during the simulation\n",
    "- A method to log each step: what action was taken, what the payoff was, the drone's position, battery level, etc.\n",
    "- A method to save all logged data to a file (CSV or JSON format) for later analysis\n",
    "- Storage of the complete history of the simulation\n",
    "\n",
    "**Example:** After running a simulation, you can review the log to see: \"At step 15, the drone was at position (7,8), chose to move right, had 67% battery remaining, and received a payoff of 0.45.\"\n",
    "\n",
    "---\n",
    "\n",
    "## PERSON 2 FILES\n",
    "\n",
    "### `backend/core/drone.py`\n",
    "\n",
    "This file contains the **Drone** class that represents the drone itself with all its properties.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- The drone's current position on the grid (x, y coordinates)\n",
    "- Battery level (current charge and maximum capacity)\n",
    "- The complete path history - every position the drone has visited\n",
    "- Energy cost per movement (how much battery each action consumes)\n",
    "- A **move** function that executes an action: updates the drone's position based on the action (if moving up, y increases by 1), consumes battery, and adds the new position to the path history\n",
    "- Functions to get current position and battery level\n",
    "- A function to check if the drone is still \"alive\" (has battery remaining)\n",
    "- A function that returns all valid actions from the current position (can't move up if there's a wall there)\n",
    "\n",
    "**Example:** A drone starts at (1,1) with 100% battery. After moving right 3 times, it's now at (4,1) with 97% battery, and its path history is [(1,1), (2,1), (3,1), (4,1)].\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/game_theory/minimax.py`\n",
    "\n",
    "This file contains the **MinimaxSolver** class that implements the minimax algorithm for adversarial environments.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- The main **minimax_decision** function that implements the core algorithm logic:\n",
    "  - For each possible drone action, it calculates the worst-case scenario (what's the minimum payoff if the environment picks its worst response?)\n",
    "  - Then it chooses the action that has the best worst-case outcome (maximize the minimum)\n",
    "- An **evaluate_action** function that, given one specific drone action, checks all possible environment responses and finds which response is worst for the drone\n",
    "- A **get_worst_case_payoff** function that calculates the guaranteed minimum payoff for an action\n",
    "- A **solve** wrapper function that orchestrates everything and returns the optimal action\n",
    "\n",
    "**Philosophy:** \"Assume the environment is trying to hurt you. What's the safest action that guarantees the best outcome even in the worst case?\"\n",
    "\n",
    "**Example:** If moving forward could give +10 payoff (if clear) or -20 payoff (if obstacle), and staying gives +2 in all cases, minimax chooses \"stay\" because its worst case (+2) is better than forward's worst case (-20).\n",
    "\n",
    "---\n",
    "\n",
    "## PERSON 3 FILES\n",
    "\n",
    "### `backend/core/sensor.py`\n",
    "\n",
    "This file contains the **DroneSensor** class that simulates the drone's vision and sensing capabilities.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- A detection range parameter (how many grid cells away the drone can see)\n",
    "- A visibility level (0 to 1, where 1 is perfect visibility, 0.5 is foggy, etc.)\n",
    "- A **scan_environment** function that returns all obstacles the drone can currently see within its range\n",
    "- A **detect_obstacles** function that checks specific directions (up, down, left, right) and reports how far away obstacles are in each direction, or None if no obstacle in that direction\n",
    "- An **update_visibility** function that adjusts visibility based on environmental conditions (fog reduces visibility)\n",
    "- A **get_observable_region** function that returns all grid coordinates the drone can currently observe\n",
    "\n",
    "**Example:** A drone at position (10,10) with range 5 can see positions from (5,5) to (15,15). If there's an obstacle at (10,13), the sensor returns \"obstacle 3 cells ahead\". If fog occurs, visibility drops to 0.6, reducing effective range.\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/game_theory/nash.py`\n",
    "\n",
    "This file contains the **NashEquilibriumSolver** class that finds stable strategy pairs.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- The main **find_nash_equilibrium** function that solves for Nash equilibrium given a payoff matrix. It uses mathematical optimization techniques to find probability distributions where neither player wants to change their strategy\n",
    "- A **support_enumeration** alternative method that tries different combinations of strategies to find equilibrium\n",
    "- A **best_response_drone** function: given what the environment is doing (its mixed strategy), what's the best thing the drone can do?\n",
    "- A **best_response_env** function: given what the drone is doing, what's the best environment response?\n",
    "- An **is_nash_equilibrium** checker that verifies if a strategy pair is actually a Nash equilibrium (neither player benefits from changing)\n",
    "- A **solve** function that computes the Nash equilibrium and samples an action from the resulting mixed strategy\n",
    "\n",
    "**Philosophy:** \"Find a stable balance where the drone's strategy is optimal given the environment's strategy, and vice versa. Nobody wants to deviate.\"\n",
    "\n",
    "**Example:** Nash might find: Drone should use (50% forward, 30% stay, 20% turn) and Environment naturally produces (80% clear, 15% obstacle, 5% fog). Neither can improve by changing unilaterally.\n",
    "\n",
    "---\n",
    "\n",
    "## PERSON 4 FILES\n",
    "\n",
    "### `backend/game_theory/bayesian.py`\n",
    "\n",
    "This file contains the **BayesianGameSolver** class that handles uncertainty and learning.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- A belief state storage - the drone's current beliefs about what type of environment it's in (is it adversarial? neutral? helpful?)\n",
    "- An **initialize_beliefs** function that sets starting beliefs before the drone has any information. Example: \"I think there's 30% chance the environment is adversarial, 50% neutral, 20% helpful\"\n",
    "- An **update_beliefs** function that uses Bayes' theorem to update beliefs after observing what actually happened. If an obstacle appeared when the drone moved forward, this increases belief that the environment is adversarial\n",
    "- An **expected_utility** function that calculates the expected payoff of an action by averaging over all possible environment types, weighted by the belief probabilities\n",
    "- A **bayesian_decision** function that chooses the action with the highest expected utility based on current beliefs\n",
    "- A **solve** wrapper function\n",
    "\n",
    "**Philosophy:** \"I don't know what kind of environment this is, but I'll maintain beliefs and update them as I learn. I'll choose actions based on expected value over my uncertain beliefs.\"\n",
    "\n",
    "**Example:** Initially believe 50-50 adversarial vs neutral. After 5 safe moves, update to 20% adversarial, 80% neutral. Now take bolder actions because you've learned the environment is probably friendly.\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/simulation/engine.py`\n",
    "\n",
    "This file contains the **SimulationEngine** class that runs the actual simulation - the \"brain\" that coordinates everything.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- References to the environment, drone, and which algorithm to use (minimax, nash, or bayesian)\n",
    "- A step counter and history log to track progress\n",
    "- Instances of all three algorithm solvers (minimax, nash, bayesian)\n",
    "- A **step** function that executes one time step of the simulation:\n",
    "  1. Selects which algorithm to use\n",
    "  2. Gets the recommended action from that algorithm\n",
    "  3. Executes the action (moves the drone)\n",
    "  4. Updates the environment state\n",
    "  5. Logs what happened\n",
    "  6. Checks if done (reached goal, crashed, or out of battery)\n",
    "- A **select_action** function that routes to the appropriate algorithm based on the current mode\n",
    "- A **run_simulation** function that runs the complete simulation from start to finish, up to a maximum number of steps, and returns the final metrics\n",
    "- A **collect_metrics** function that gathers all results at the end\n",
    "- A **reset** function to restart the simulation\n",
    "\n",
    "**Example:** You tell the engine \"use minimax algorithm\". It runs step-by-step: gets minimax action, moves drone, checks status, logs data, repeats until goal reached or failure. Returns: \"Reached goal in 47 steps, used 68% battery, 0 collisions.\"\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/simulation/metrics.py`\n",
    "\n",
    "This file contains the **PerformanceMetrics** class that measures and evaluates how well the drone performed.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- A **calculate_success_rate** function that checks: Did the drone reach the goal? Returns True/False\n",
    "- A **calculate_path_efficiency** function that compares the actual path length to the optimal (shortest) path length. Returns a ratio (1.0 = perfect, 0.5 = took twice as long as needed)\n",
    "- A **calculate_pareto_optimality** function that checks if the solution is Pareto optimal by comparing it to other solutions. Returns whether any solution exists that's better in one dimension without being worse in another\n",
    "- A **calculate_security_level** function that computes the worst-case guaranteed payoff - the minimum the drone can expect no matter what happens\n",
    "- A **calculate_nash_efficiency** function that compares the payoff at Nash equilibrium to the theoretically optimal payoff (price of anarchy)\n",
    "- A **generate_report** function that combines all metrics into a comprehensive dictionary with success status, path length, battery used, number of collisions, computation time, whether Pareto optimal, security level, and Nash efficiency\n",
    "\n",
    "**Example:** Final report might say: \"Success: Yes, Path length: 52 (optimal was 45, efficiency: 86.5%), Battery used: 71%, Collisions: 0, Pareto optimal: True, Security level: 0.42, Nash efficiency: 0.89\"\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/evaluation/compare.py`\n",
    "\n",
    "This file contains the **AlgorithmComparator** class that compares all three algorithms against each other.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- Storage for the environment and drone to test with\n",
    "- A results dictionary to store outcomes from each algorithm\n",
    "- A **run_all_algorithms** function that:\n",
    "  - Runs minimax multiple times (e.g., 10 trials)\n",
    "  - Runs nash multiple times\n",
    "  - Runs bayesian multiple times\n",
    "  - Stores all results for statistical comparison\n",
    "- A **compare_metrics** function that performs statistical analysis on the results: calculates mean, standard deviation, min, max for each metric across algorithms\n",
    "- A **visualize_comparison** function (optional) that creates charts comparing the algorithms\n",
    "- A **generate_comparison_report** function that produces a final report answering: \"Which algorithm was best overall? Which was fastest? Which used least battery? Which was safest?\"\n",
    "\n",
    "**Example:** After running 10 trials of each, the report might say: \"Minimax: 90% success rate, average 63 steps, 72% battery. Nash: 95% success, average 58 steps, 68% battery. Bayesian: 88% success, average 61 steps, 70% battery. Winner: Nash equilibrium.\"\n",
    "\n",
    "---\n",
    "\n",
    "### `backend/main.py`\n",
    "\n",
    "This is the **entry point** - the file you actually run to start everything.\n",
    "\n",
    "**What it contains:**\n",
    "\n",
    "- The main function that orchestrates the entire project\n",
    "- Setup code that:\n",
    "  1. Creates the environment (20x20 grid, adds obstacles, sets goal and start positions)\n",
    "  2. Creates the drone (sets starting position and battery)\n",
    "  3. Runs a single algorithm test (e.g., test minimax alone first)\n",
    "  4. Runs the comparison of all algorithms\n",
    "  5. Calculates and displays final metrics\n",
    "  6. Prints results to the console\n",
    "- The `if __name__ == \"__main__\"` block that runs when you execute the file\n",
    "\n",
    "**Example flow:** You run `python main.py` → It creates a 20x20 grid with obstacles, creates a drone at (1,1), tests minimax (prints \"Minimax reached goal in 47 steps\"), then runs all three algorithms 5 times each, and finally prints \"Nash performed best with 95% success rate.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of How Files Work Together\n",
    "\n",
    "1. **Person 1's files** create the foundation: the world (environment), the rules (strategies), and the scoring system (payoff)\n",
    "\n",
    "2. **Person 2 and 3's files** build the intelligence: minimax makes safe decisions, nash finds balance, and sensors provide awareness\n",
    "\n",
    "3. **Person 4's files** bring it all together: bayesian adds learning, the engine runs simulations, metrics measure performance, comparison evaluates algorithms, and main.py starts everything\n",
    "\n",
    "**Analogy:** It's like building a video game: Person 1 makes the map and rules, Person 2 makes an AI that's very defensive, Person 3 makes an AI that's balanced, Person 4 makes an AI that learns, runs the game, keeps score, and determines who wins!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
